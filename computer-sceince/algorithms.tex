
\chapter{Algorithms}
\section{Polynomial evaluation}
\red{add begining}
\begin{defn} \label{def:poly-eval}
  For any polynomial $P$ in $Z_q[X]$ and any element $a$ in $Z_q$,
  the substitution of $X$ with $a$ in $P$ defines an element of $Z_q$, which is denoted $P(a)$.
  This element is obtained by carrying on in $Z_q$ after the substitution of the operations indicated by 
  the expression of the polynomial.
  We call this action polynomial evaluation of $P$ at $a$.
\end{defn}
\red{add horners rule/ horners algorithm.}
\red{consider adding Multipoint evaluation.}

\section{Polynomial Identity Testing (SZ)}
In this section, we talk about identity testing, 
and some issues involving this subject.

Given two polynomials $P(x)$ and $Q(x)$ of the same degree $d$, 
how do we prove these are the same polynomials?
We can calculate it and check $d$ points on each polynomial 
since each polynomial can be solely defined by its point-value representation.
\red{provide proof.}


Schwartz-Zippel \cite{SZ} made this notion much more efficient 
(and complete since they discuss multivariate polynomials).
By choosing a random point for evaluating each polynomial, we can 
be almost entirely certain 
whether the polynomials are equal or not.

\begin{theorem}
 Let $P(x)$ be a non-zero polynomial of total degree $d$ over 
 an integral domain $R$. Let $S$ be a finite subset of $R$, and let $r$
 be a selected random number from $S$. Then
  $\Pr [P(x)=0]\le \frac{deg(p(x))}{|S|}$.
\end{theorem}
\begin{proof}
  $P(x)$ is a polynomial, from the fundamental theorem of algebra\cite{Aigner2010} it has 
 at most $d$ roots. Thus, choosing some random point $r$,
  $\Pr [P(r)=0]\le \frac{d}{|S|}$. 
\end{proof}


Given this notion, one can easily extrapolate this idea and create a method 
to check whether $P(x)\cdot Q(x)$ is equal to some given $Z(x)$: 
choose a random number $r$, and evaluate $P(r)\cdot Q(r) - Z(r)$.
if this expression equates to $0$, then $P(x)\cdot Q(x)=Z(x)$ with overwhelming 
probability (depending on the size of $Z_q$).

\subsection{Troubled waters: $Z_q[X]/F(X)$}
Given the above method, we can investigate new and upcoming ideas on numerous
problems (see  \cite{sz-usage2} and \red{fpir- to be published}).
The unlucky ones might encounter a troubling problem.
While SZ works quite well over fields and rings,
One would quickly find out that more complex algebraic structures do not 
bend so easily to this powerful technique.

For instance, consider the following quotient polynomial rings $R_q=Z_q[X]/(X^n+1)$,
choose two random polynomials from $R_q$ and try the method of testing above.
It is highly likely that for some random $r$, $P(r)\cdot Q(r) \ne (PQ \mod (X^n+1))(r)$.

Before we explain the source of the issue, let us define two concepts.
\begin{defn}
  Given $a\in Z_q$, we denote the function $f_a: R_q \to Z_q$ as the \bf{polynomial evaluation map},
  which is defined as follows: $f_a(P)\to P(a)$ \autoref{def:poly-eval}.
\end{defn}

\begin{defn}
  If $R$ and $S$ are rings, then a ring homomorphism is a function $f:R \to S$
that preserves addition, multiplication and multiplicative identity; that is
$f(a+b) = f(a)+f(b)$, $f(a\cdot b) = f(a) \cdot f(b)$, $f(1_R)=1_s$. 
\end{defn}

Given these two definitions, we can now fully express what occured above. 
Not every point $r\in Z_q$ can be used to create a polynomial evaluation map $f_r$ such that 
$f_r$ is a ring homomorphism. That is $f_r(P\cdot Q) \ne f_r(P)\cdot f_r(Q)$.

\paragraph{Finding valuable points}
Fortunately, there are some points $r\in Z_q$ which can be used to create polynomial evaluation maps that 
are ring homomorphisms.
\begin{thm}
  Let $R_q=Z_q[X]/F(X)$ and let $r\in Z_q$. If $F(r)=0$ then $f_r$ is a ring homomorphism.
\end{thm}
\begin{proof}
  $f_r$ preserve the multiplicativeidentity: $f_r(1_{R_q}) = 1(r)=1$.
  Every $f_r$ preserver addition.
  preserving multiplication:
  Let there be $P(X), Q(X) \in R_q$. 
  By definition $P(X)\cdot Q(X) \mod F(X) = P(X)\cdot Q(X) \lfloor \frac{P(X)\cdot Q(X)}{F(X)} \rfloor \cdot F(X)$,
  so $f_r(P(X)\cdot Q(X))= f_r(P(X))cdot f_r(Q(X)) - f_r(\lfloor  \frac{P(X)\cdot Q(X)}{F(X)} \rfloor)\cdot f_r(F(X))$.
  Because $f_r$ is an evaluation map, $f_r(\lfloor  \frac{P(X)\cdot Q(X)}{F(X)} \rfloor)=z$ where is some unknown integral from $Z_q$.
  then $f_r(P(X)\cdot Q(X)) =P(r)\cdot Q(r) - z \cdot 0= P(r)\cdot Q(r)$, as wanted.
\end{proof}

So, by finding roots of $F(X)$ one can find possible values where SZ still works. 
unfortunately, this might be a smaller set than we thought.

\section{FFT, DFT and NTT}
\subsection{representations of a polynomial}
we can either use \emph{coefficient representation} or \emph{Point-value representation}.

\subsubsection*{coefficient representation.}
A coefficient representation of a polynomial $P(x)$ is a vector of coefficients 
$a=(a_0,a_1,\dots, a_{n-1})$.
In coefficient representation, we can evaluate or add a polynomial in $O(n)$ time. 
unfortunately multiplying two polynomials takes $O(n^2)$.
The operation of multiplying two polynomials $a,b$ is called convolution $c=a\otimes b$.


\subsubsection*{Point-value representation.}
A point-value representation of a polynomial $P(x)$ of degree-bound $n$ is a set of $n$ point-value pairs.
$$ \{ (x_0,P(x_0)),\dots , (x_{n-1}, P(x_{n-1})) \}$$

\begin{theorem}[Uniqueness of an interpolating polynomial]
  For any set $ \{ (x_0,P(x_0)),\dots , (x_{n-1}, P(x_{n-1})) \}$ of $n$
  point-value pairs such that all the $x_k$ values are distinct,
  there is a unique polynomial $P(x)$ of degree-bound $n$ such that
  $y_k = P(x_k)$. for $k\in[n]$ 
\end{theorem}

Due to the above theorem, we can see that this representation defines a polynomial well. 
And one can move between both representations.

Because $C(X)=A(X)B(X)$ for any $x_j$, then $C(x_j)=A(x_j)B(x_j)$. Meaning 
multiplication and addition in this representation cost $O(n)$.
 
\section*{FFT}
Luckily there is a way to move to and from both representations quickly using the \emph{roots of unity}.

\begin{defn}
An $n$th root of unity is a complex number $\omega$ such that
$\omega^n =1$.
\end{defn}

There are exactly $n$ roots of unity $e^{2\pi i k/n}$ for $k\in[n-1]$ 
($e^{iu}= \cos{u}+i \sin{u}$).

The value $\omega_n=e^{2\pi i/n}$ is the \emph{principal nth root of unity}.
all other roots of unity are powers of $\omega_n$:
 $\omega_n^0, \omega_n^1, \omega_n^2,\dots, \omega_n^{n-1}$.

FFT takes special advantage that these values have special properties.
\begin{lemma}[roots of unity]\label{roots-of-unity-property}
If $z$ is an nth root of unity, then $ a \equiv b \mod n \Rightarrow z^a = z^b$.
\end{lemma}
meaning that $\omega_n^a = \omega_n^b$. one can label specific points on the unit circle over $\C$.

\paragraph{Why is that important?}

In the simplest of cases, we are going to evaluate the polynomial over the following
points $ \{\omega_n^0, \omega_n^1, \omega_n^2,\dots, \omega_n^{n-1}\}$.
Let us view the polynomial evaluation over $n$ such points as a matrix, this presentation
might help the reader see some similarities:


$$
\begin{pmatrix}
  y_0\\
  y_1\\
  \vdots \\
  y_{n-1}\\
\end{pmatrix}
=
\begin{pmatrix}
  1 & x_0 & x_0^2 ,\dots\\
  1 & x_1 & x_1^2, \dots,\\
  \vdots & \vdots & \vdots & \ddots & \\
  1 & x_{n-1} & x_{n-1}^2, \dots,\\
 \end{pmatrix}
 \cdot 
\begin{pmatrix}
  a_0\\
  a_1\\
  \vdots \\
  a_{n-1}\\
\end{pmatrix}
 $$

 Now, let us assign the values:

 \label{dft-matrix}
 $$
\begin{pmatrix}
  p(\omega_n^0)\\
  p(\omega_n^1)\\
  \vdots \\
  \omega_n^2\\
\end{pmatrix}
=
\begin{pmatrix}
  1 & (\omega_n^0)^1 & (\omega_n^0)^2 ,\dots ,(\omega_n^0)^{n-1}\\
  1 & (\omega_n^1)^1 & (\omega_n^1)^2, \dots, (\omega_n^1)^{n-1}\\
  \vdots & \vdots & \vdots & \ddots & \\
  1 & (\omega_n^{n-1}) & (\omega_n^{n-1})^2, \dots,(\omega_n^{n-1})^{n-1}\\
 \end{pmatrix}
 \cdot 
\begin{pmatrix}
  a_0\\
  a_1\\
  \vdots \\
  a_{n-1}\\
\end{pmatrix}
 $$

 The keen looker can see that we can reuse values in this matrix.
 for example, let us look at $(\omega_n^{n-1})^{n-1}$ using \ref{roots-of-unity-property}
 $(\omega_n^{n-1})^{n-1} = (\omega_n)^{n*n-2n+1}=(\omega_n)^{0+0+1}$.  

 Thus we gain a matrix with heavy symmetry, which the FFT can use.



\subsubsection*{fft main idea}
The FFT method employs a divide-and-conquer strategy, 
using the even-indexed and odd-indexed coefficients of $A(x)$
separately to define the two new polynomials $A^{[0]}(x)$ and $A^{[1]}(x)$ of degree-bound $n/2$.

\begin{align}
  &A^{[0]}(x) =  a_0 +a_2x + a_4x^2 + \dots + a_{n-2}x^{n/2-1}\\
  &A^{[1]}(x) =  a_1 +a_3x + a_5x^2 + \dots + a_{n-1}x^{n/2-1}.
\end{align}


It follows that $A(x)=A^{[0]}(x^2) + x\cdot A^{[1]}(x^2)$.

Thus, the problem of evaluating $A(x)$ at 
$\omega_n^0, \omega_n^1, \omega_n^2,\dots, \omega_n^{n-1}$ 
reduces to evaluating the degree-bound $n/2$ polynomials  $A^{[0]}(x)$ and $A^{[1]}(x)$ at the points
$$(\omega_n^0)^2, (\omega_n^1)^2,\dots, (\omega_n^{n-1})^2$$
and then combining the results according to $A(x)=A^{[0]}(x^2) + x\cdot A^{[1]}(x^2)$.


Using the properties of the root of unity, 
the above values consist not of $n$ distinct values but only of the $n/2$
 complex ($n/2$)th roots of unity, with each root occurring exactly twice!
These subproblems have the same form as the original problem but are half the size.
We can continue in this manner until we evaluate the full polynomial on $n$ distinct values.


\subsection{putting it all together}
\red{there might be some miscalculations here, but the idea is solid.}
As we've seen we can create a matrix with heavy symmetry, called the DFT matrix. 
now, we'd want to capitalize on that and create a recursive algorithm.
To do so, instead of the regular polynomial assignment we've seen above using a matrix,  
we'll shuffle things a bit according to the divide-and-conquer approach from above
$$
F_n \cdot p(x) = 
\begin{pmatrix}
  I_{ \frac{n}{2}} & D_{\frac{n}{2}} \\
  I_{ \frac{n}{2}} & -D_{\frac{n}{2}}
 \end{pmatrix}
 \cdot 
 \begin{pmatrix}
  F_{ \frac{n}{2}} & 0 \\
  0 & F_{ \frac{n}{2}} 
\end{pmatrix}
\cdot 
\begin{pmatrix}
  p_{\text{even coefficient}} \\
  p_{\text{odd coefficient}}
\end{pmatrix}
 $$

 where $F_n$ is the matrix of assignment we've seen in ~\ref{dft-matrix}, and 
 $D_n$ is the following diagonal matrix $$
 \begin{pmatrix}
  1 & 0 &0  & \dots& 0 \\
  0 & \omega_n^1 &0  & \dots& 0 \\
  1 & 0  &\omega_n^2  & \dots& 0 \\
  \vdots & \vdots & \vdots & \vdots &  \\
  0 & 0  &0  & \dots& \omega_n^n-1 \\
 \end{pmatrix} $$


 We can continue this process recursively, to get what we want.
 In each recursive level, the algorithm does at most $O(n)$ work (i.e., multiplying by sparse matrices),
  and due to the logarithmic depth of the recursion, we get $O(n\log{n})$.
x
\label{algorithm:fft}
  \begin{lstlisting}[language=Python,
    caption={fft algorithm in python}
  ]
import numpy as np

def fft(p: np.ndarray) -> np.ndarray:
    n = len(p)
    if n == 1:
        return p
    omega = np.exp(2 * np.pi * 1j / n)

    p_even, p_odd = p[::2], p[1::2] 
    ye, yo = fft(p_even), fft(p_odd)

    # mult the structured left matrix with the recursive result.
    # O(n) work comes from here.
    y = np.zeros(n, dtype=np.complex128)
    for k in range(n // 2):
        y[k] = ye[k] + omega ** k * yo[k]
        y[k + n // 2] = ye[k] - omega ** k * yo[k]
    return y
    \end{lstlisting}
    
    \begin{remark}[Summary]
      Notice that to achieve this result, we needed to reduce the polynomial into
      even and odd parts, and then adjust the assignment matrix using
      the properties of roots of unity. Each part is important in its own way.
    \end{remark}
\subsection*{Inverse FFT}\label{ifft}

The DFT matrix is invertible.
Thus one can solve for the vector of coefficients,
by simply inverting the matrix in~\ref{dft-matrix}.

Fortunately, with little tricks we can adjust the inverse of the DFT matrix such that
we can use the same algorithm as above, but instead of
$\omega^k$ we'll use $\omega^{-k}$, and after we are done with the FFT
algorithm, we'll need to normalize it by dividing every coefficient element
by $N$ the size of the polynomial.

 \begin{corollary}
  For FFT, DFT, and NTT we must use specific values to gain speedy interpolation and
  evaluation of the polynomial. 
 \end{corollary}{Conclusin:}
 
\section*{Number Theoretic Transform (NTT)}

NTT is a specialized version of the discrete Fourier transform,
in which the coefficients of the polynomial are members of a finite field (or ring)
 containing the right roots of unity.
It can be viewed as an exact version of the complex DFT, avoiding 
round-off errors for exact convolutions of integer sequences.
While Gauss used similar techniques already in~\cite{gaussfft}, laying the groundwork 
for modern FFT algorithms to compute the DFT, and therefore the NTT, 
is usually attributed to Cooley and Tukey's seminal paper \cite{fft}.

NTT can use the same FFT algorithms but requires a 
different DFT matrix~\ref{dft-matrix}.

This fact lies in the foundational theorem of NTT:
\begin{thm}[\cite{ntt}]
  A length $N$ transform having the DFT structure will
  implement cyclic convolution if and only if there exists an 
  inverse of N and an element $\alpha$,
  a root of unity of order $N$ , i.e., $N$ is the least 
  positive integer such that $\alpha^N =1$.
\end{thm}
This is a very general result applying to both \emph{rings and fields} that are
finite or infinite.

It is important to note that $N$ also applies to the length of our polynomial.

\subsection{Preliminaries}
I will describe everything in terms of fields, but essentially all of this can be easily generalised to rings 
using the Chinese Remainder Theorem (CRT), as shown below.
\paragraph{\bf Finding Roots of Unity (In Field)}
As described above, an $n$-th root of unity, is an element $\alpha\in Z_q$ 
such that $\alpha^n =1  \mod q $, and for every $0<k<n$ $\alpha^k \neq 1 \mod q$.

\paragraph{Finding roots of unity in a field}

\begin{thm}
  Let $g\in GF(q)$ be a generator, thus
  $\omega_0=g^{(q-1)/n}$ is an $n$-th root of unity.
\end{thm}
\begin{proof}
  relying on \autoref{fermats-little-theorem}, we know that $g^{q-1}=1 \mod q$.
  So $\omega_0^n=g^{(q-1)/n}=1$, now we just need to explain away why $\omega_0^k \ne 1$ for 
  every $0<k<n$.
  Let us assume in contradiction that there exists some $k<n$ such that $\omega_0^k=1$, then 
  $\omega_0^k=g^{(q-1/n\cdot k)}=1$, more specifically, $g^c=1$ for $c<q-1$. thus, we have a 
  cycle $(1,g,g^1,...g^{c-1})$, which is less than $q-1$ elements, meaning that $g$ is not 
  a generator, in contradiction.
\end{proof}

Now that we found the first root of unity, we can find the rest by computing the powers of $\omega_0$ up to $n$.
\paragraph{\bf Finding Inverse}
\red{TODO:} explain Bezout's algorithm.


\paragraph{\bf Chinese Remainder Theorem (CRT) For Dummies}
CRT is a way of representing elements $x\in Z_q$ (
  where $q=p_1,p_2,\dots,p_k$ and $p_i\in\text{Primes}$)
such that the $x_{CRT}$ is a single and unique representation of $x$ using the 
remainders of each of the prime factors constructing $q$:

$$ x_{CRT} = (x\mod p_1, x\mod p_2 ,\dots x\mod p_k ) $$



Because it is a unique representation, given $x_{CRT}$ and the list of
 primes $(p_1,p_2,\dots,p_k)$ there is an algorithm to solve for $x \in Z_q$
 (i.e., find $x \mod q$).


Another important thing to notice is that if  $x=1 \mod q$
 then $x_{CRT}= (1,1,1,\dots,1)$.


There is more to it honestly, but for our cause, it's enough.

\subsection{The Algorithm}

\paragraph[NTT Over Finite Fields]{\bf NTT Over Finite Fields} \label{ntt-field}
When working over a field $GF(p)$ such that $p$ is prime,
the maximal $N$, the array of coefficients representing a
polynomial, is $N_{max}=p-1$, and $N$ is a valid size for NTT
 in this field iff $N|p-1$ (\cite[III. number theoretic transforms]{ntt}). 


To compute the NTT over $GF(p)$, find the $n$th root of unity in $GF(p)$,
 then run any FFT algorithm mod $p$ using the roots
of unity for each level of the recursion.

That is, find $\omega_n=\alpha$ such that $\alpha^N=1 \mod p$, 
then run the (almost) same algorithm in \nameref{algorithm:fft}, 
but change line 7 such that omega is now the $n$-th root of unity in $GF(p)$, and
all operations in the algorithm are now $\mod p$.

\paragraph{\bf iNTT}
Just like in \nameref{algorithm:ifft}, but to calculate $\omega^-1$, we'll
use Bezout's algorithm to find the inverse of every element ($x^-1$).



\paragraph[short]{\bf NTT over Rings}
Let $Z_M$ represent a ring of integers ${0,1,2,\dots , M-1}$,
with arithmetics mod $M$.
Where $M$ is constructed the following unique prime factorization
$M=p_{1}^{r_1}\cdot  p_{2}^{r_2} \cdots p_{\ell}^{r_\ell}$
and $p_i$'s are distinct primes (see \cite[NTT sec. III][]{ntt}
 for further explanations).

For NTT to work, the following must hold $\bf N|L(M)$
where $L(M)$ is the greatest common
divisor (gcd) of the $(p_{i}-1)$. That is, 
$L(M):= gcd\{p_1-1,p_2-1,\dots\}$

essentially it means we want $ N$ to divide $p_i-1$.

\begin{corollary}
  $L(M)=N_{max}$. i.e., it defines the maximal $N$ 
  such that we can apply NTT.  
\end{corollary}

Now, Once $M$ and $N$ are known, to perform NTT we just need to be able to find 
$N$-th root of unity. but it's quite hard to find one in $Z_M$.
Luckily, we know it is feasible to do so in a prime field. 
So, let's find $\alpha_i$, the $N$-th root of unity for every $p_i$.
Once we find all of these roots of unity, we know that $\alpha_i^N=1 \mod p_i$ for every $i$.
In other words, $N$ is the least positive integer such that
\begin{align*}
  &\alpha_{CRT}^N = (\alpha_1^N \mod p_1=1, \alpha_2^N \mod p_2=1,\dots)= (1,1,\dots)
\end{align*}

and according to CRT $\alpha^N=1 \mod M$. Thus, solving for $\alpha$ 
means we'll be able to find the $N$-th root of unity in $Z_M$.

The algorithm remains the same as in \nameref{ntt-field} from here on.

\subsection{Special Features}
Applying the NTT transform provides a cyclic convolution, computing 
$c = a \cdot b \mod (X^n + 1)$  with two polynomials $a$ and $b$ would require 
applying the NTT of length $2n$ and thus $n$ zeros to be appended to each input;
this effectively doubles the length of the inputs and also requires the computation of 
an explicit reduction modulo $X^n +1$. 
To avoid these issues, one can exploit the \emph{negative wrapped convolution} \cite{negantt}.

This exploitation allows us to perform $NTT$ without expanding to $2n$ parameters!
meaning we can send less bandwidth over the network, and perform fewer computations too. 
This trick can be seen in \cite{SEAL}.

\section{Reed-Solomon}
It is fairly simple, but effective algorithm to encode words and be able to deal with erasurs.
\subsection{Naive solution}
assuming we decide we want our words to be of size $k=4$, and we add additional two redundancies $n=6$. 
Thus to send it we'll do the following: given a word vector, we'll treat it as coefficients values of a 
polynomial of degree $k-1$ (because we have k coefficients its a k-1 degree poly), then to encode it, we'll evaluate it on the following $x$ values: $(-1,0,1,2,\dots n)$.


Now given the 6 values that we received over the network we can decode it as follows: 
We go over every group of 4 points, and try to reconstruct the polynomial. 

the polynomial we've reconstructed the most is the correct one.

\subsection{Systematic Reed-Solomon}
Assuming we want to be able to correct up to $s$ values, we'd need at least $2s$ redundant symbols.
So if $k=4$, and $n=6$, we can only recover a single byte.

It won't work for larger values, like $k=223$, $n=255$ which is a popular value for reed-solomon.
(Meaning we add 32 more symbols to our message, so we can repair 16 symbols in total).

The previous algorithm will work, but we'd need to send different values then 
the original message. It would be best to send $Message||\text{addition code word}$.
that way,we might be able to avoid reconstruction if we can find quickly that there was no error.

this is called a Systematic code. 
