\part{Algorithms}

\chapter{Finite Fields}\label{finite-fields}
\section{GCD and Extended Euclidean Algorithm}

\subsection{GCD}
The greatest common divisor (GCD) of two integers
 $a$ and $b$ is the largest integer that divides both $a$ and $b$.
 While seemingly simple, the GCD is a fundamental concept in number theory
 and used extensively when working with integers over finite fields.


 \begin{lstlisting}[language=Python,
  caption={GCD algorithm in python}
]

def gcd(a, b):
    while b != 0:
        a, b = b, a%b

    return a

 \end{lstlisting}

This simple algorithm works since 
$a=b*q+r$ where $r$ is some remainder. The $GCD(a,b)=x$
must divide both of these numbers, thus it should be able to divide $b$, and $r$ (
otherwise, $a/x = \frac{b*q+r}{x}=\frac{b*q}{x}+\frac{r}{x}$, and if $\frac{r}{x}$ has a remainder 
then it is a contradication to it being a gcd of $a$).
Thus, $GCD(a,b)=GCD(b,r)$, and we can continue the algorithm until the smaller 
value is $0$.


\subsection{Extended Euclidean Algorithm (inverse algorithm)}
\label{sec:finite-field:extended-euclidean}
The Extended Euclidean Algorithm is an important algorithm in any number theory course.
This algorithm is used to find the inverse of an element in a finite field.

The algorithm is based on Bezout's identity, which states
that for any two integers $a$ and $b$, there exist integers $x$ and $y$ such that
$ax+by=GCD(a,b)$. As a result of this identity, we can find the inverse of an element
in a finite field $GF(q)$. How can we use this to find the inverse?
Welp, in a finite field $q$ is a prime right? so $GCD(a,q)=1$ for every $a\in GF(q)$.
As a result, we have $ax+qy=ax+0=GCD(a,q)=1$ (where $qy=0$ since we are in $GF(q)$), and thus $ax=1 \mod q$. and luckily, the 
extended Euclidean algorithm can find this $x$ and this $y$ (we toss the $y$).


The algorithm is basically applying the GCD algorithm, but keeping track of the coefficients
$x$ and $y$ in the process.

\begin{lstlisting}[language=Python,
  caption={Extended Euclidean Algorithm in python}
]
def extended_euclidean(a, b):
    x0, x1, y0, y1 = 1, 0, 0, 1
    while b != 0:
        q, a, b = a // b, b, a % b # advancing the algorithm by one step.

        # updating the x coefficients. try doing gcd by hand to understand this.
        x0, x1 = x1, x0 - q * x1 
        y0, y1 = y1, y0 - q * y1

    return a, x0, y0

\end{lstlisting}


\todo{ensure the algorithmis okay. taken form chatgpt.+ write intuition}

\section{Chinese Remainder Theorem}

\section{Generator}

\section{Roots of Unity}



\chapter{Polynomial Algorithms}\label{polynomials}


% \section{Coefficient Representation}\label{polynomials:coeffcient-pres}
% A coefficient representation of a polynomial $P(x)$ is a vector of coefficients 
% $a=(a_0,a_1,\dots, a_{n-1})$.
% In coefficient representation, we can evaluate or add a polynomial in $O(n)$ time. 
% unfortunately multiplying two polynomials takes $O(n^2)$.
% The operation of multiplying two polynomials $a,b$ is called convolution $c=a\otimes b$.
% \todo{Define coefficient representation mroe formally}

\section{Polynomial evaluation}
Polynomial evaluation is the process of computing the value of a polynomial at a given point.
This is a fundamental operation in many algorithms and applications. 
There are several methods to evaluate polynomials, with varying levels of efficiency.
Before we delve into the methods, we define what is evaluation of a polynomial and a polynomial evaluation map.

\begin{defn} \label{def:poly-eval}
  For any polynomial $P$ in $Z_q[X]$ and any element $a$ in $Z_q$,
  the substitution of $X$ with $a$ in $P$ defines an element of $Z_q$, which is denoted $P(a)$.
  This element is obtained by carrying on in $Z_q$ after the substitution of the operations indicated by 
  the expression of the polynomial.
  We call this action polynomial evaluation of $P$ at $a$.
\end{defn}

Now that we've defined the evaluation of a polynomial, it is important to state that formally, polynomials aren't functions. 
Thus to evaluate a polynomial at a point, we need to define a function that receives a polynomial and a point and returns the evaluation.
\begin{defn}
  A polynomial evaluation map is a function $f_r:Z_q[X]\to Z_q$ such that for every polynomial $P$ and $r\in Z_q$,
  $f_r(P)=P(r)$.
\end{defn}

This notation of polynomial evaluation maps is crucial to reason about polynomials, their evaluation, 
and proving properties. For instance, assume a matrix $M$ and a vector $v$ with polynomials entries in $\Z_q[x]$.
It isn't clear straight away that $f_r(M\cdot v) = f_r(M)\cdot f_r(v)$. In fact, there are algebraic 
structures where this above equation is false. 
Thus, to claim the above statement we need to define ring homomorphisms, and prove that $f_r$ is a ring homomorphism.

\begin{defn}
  A ring homomorphism is a function $f:R\to S$ between two rings $R$ and $S$ such that for every $a,b\in R$:
  \begin{align}
    &f(a+b)=f(a)+f(b)\\
    &f(a\cdot b)=f(a)\cdot f(b) \\
    &f(1_R)=1_S
  \end{align}
   Where $1_R$ and $1_S$ are the multiplicative identities of $R$ and $S$ respectively.
\end{defn}

Fortunately, for every point in $\Z_q[X]$, we can construct a polynomial evaluation map that is 
a ring homomorphism (prove this to ensure you understand the reasoning behind it).
Consequently, we can assert that $f_r(M\cdot v) = f_r(M)\cdot f_r(v)$.

\subsection{Evaluation Algorithms}
Evaluating a polynomial at a point is a straightforward process.
Substituting the point for $X$ in the polynomial gives the result. 
However, Horner's rule offers a more efficient method.

Horner's rule reduces the number of multiplications required to evaluate the polynomial.
It rewrites the polynomial in a nested form, making computation more efficient.

To apply Horner's rule to evaluate a polynomial $P(x) = a_0 + a_1x + a_2x^2 + \dots + a_nx^n$ at a 
point $x = r$, follow these steps:

\begin{lstlisting}[language=Python,
  caption={Horner's rule algorithm in python}
]
def horner(p: List[int], r: int) -> int:
    result = p[-1]
    for i in range(len(p) - 2, -1, -1): # iterate from n-1 to 0
        result = result * r + p[i] # result = ((((....)x+a_3)x+a_2)x+a_1)x +a_0 = p(r)
    return result
\end{lstlisting}\label{algorithm:polynomial:evaluation}


\section{Coefficient vs Point-Value representation}\label{polynomials:point-val-representation}
We've defined one representation for polynomials, the coefficient representation~\ref{polynomials:coeffcient-pres},
But there is another representation, which is used in numerous algorithms (e.g.,~\ref{schawtz-zippel},\ref{polynomial:multiplication}):


A point-value representation of a polynomial $P(x)$ of degree-bound $n$ is a set of $n+1$ point-value pairs.
$ \{ (x_0,P(x_0)),\dots , (x_{n}, P(x_{n})) \}$, which uniquely defines it.

\begin{theorem}[Uniqueness of an interpolating polynomial]
  For any set $ \{ (x_0,P(x_0)),\dots , (x_{n}, P(x_{n})) \}$ of $n+1$
  point-value pairs such that all the $x_k$ values are distinct,
  there is a unique polynomial $P(x)$ of degree-bound $n$ such that
  $y_k = P(x_k)$. for $k\in[n]$ 
\end{theorem}

\begin{proof}
  \sloppy{
  So let there be a polynomial $p(x) \neq 0$ of degree 
  $n$ and a set $ \{ (x_0,P(x_0)),\dots , (x_{n}, P(x_{n})) \}$
  of points representing it. Let us assume in contradication that there exists $q(x)\neq p(x)$ and it also happens
  that $p(x_0)=q(x_0), \dots, p(x_n)=q(x_n)$. That is, there is another polynomial that is evaluated to the same points,
  making the representation non-unique.

  So if such $q(x)$ exists, we can create a new polynomial $(p-q)(x)$, notice that $(p-q)$ is also of degree $n$.
  $x_i$ is a root $0\le i\le n$ of $(p-q)(x)$, since $(p-q)(x_i)=p(x_i)-q(x_i)$, thus it has $n+1$ roots.
  This is a contradication to the fundamental theorem of algebra \todo{cite}, that is, a polynomial $p(x)\neq 0$ 
  of degree $n$ has at most $n$ roots. }
\end{proof}

Now that we know that this representation is unique, I'll claim without proof that
there is a mapping that is both injective and surjective (bijective) from both representations.
Thus these both know representations are isomorphic.

\section{Polynomial Modulo}
\todo{TODO}
\section{Polynomial Identity Testing (SZ)}\label{schawtz-zippel}
In this section, we talk about identity testing, 
and some issues involving this subject.

Given two polynomials $P(x)$ and $Q(x)$ of the same degree $d$, 
how do we prove these are the same polynomials?
We can calculate it and check $d$ points on each polynomial 
since each polynomial can be solely defined by its point-value representation\ref{polynomials:point-val-representation}.


Schwartz-Zippel \cite{SZ} made this notion much more efficient 
(and complete since they discuss multivariate polynomials).
By choosing a random point for evaluating each polynomial, we can 
be almost entirely certain 
whether the polynomials are equal or not.

\begin{theorem}
 Let $P(x)$ be a non-zero polynomial of total degree $d$ over 
 an integral domain $R$. Let $S$ be a finite subset of $R$, and let $r$
 be a selected random number from $S$. Then
  $\Pr [P(x)=0]\le \frac{deg(p(x))}{|S|}$.
\end{theorem}
\begin{proof}
  $P(x)$ is a polynomial, from the fundamental theorem of algebra\cite{Aigner2010} it has 
 at most $d$ roots. Thus, choosing some random point $r$,
  $\Pr [P(r)=0]\le \frac{d}{|S|}$. 
\end{proof}


Given this notion, one can easily extrapolate this idea and create a method 
to check whether $P(x)\cdot Q(x)$ is equal to some given $Z(x)$: 
choose a random number $r$, and evaluate $P(r)\cdot Q(r) - Z(r)$.
if this expression equates to $0$, then $P(x)\cdot Q(x)=Z(x)$ with overwhelming 
probability (depending on the size of $Z_q$).

\subsection{Troubled waters in $Z_q[X]/F(X)$}
Given the above method, we can investigate new and upcoming ideas on numerous
problems (see  \cite{sz-usage2} and \red{fpir- to be published}).
The unlucky ones might encounter a troubling problem.
While SZ works quite well over fields and rings,
One would quickly find out that more complex algebraic structures do not 
bend so easily to this powerful technique.

For instance, consider the following quotient polynomial rings $R_q=Z_q[X]/(X^n+1)$,
choose two random polynomials from $R_q$, and try the method of testing above.
It is highly likely that for some random $r$, $P(r)\cdot Q(r) \ne (PQ \mod (X^n+1))(r)$.

This issue arises because not every point $r\in Z_q$ can be used to construct a
polynomial evaluation map $f_r$ (refer to \autoref{def:poly-eval}) 
over $R_q$ such that $f_r$ is a ring homomorphism. 
Consequently, certain values of $r$ will not be compatible with the Schwartz-Zippel method.
In other words, $f_r(P\cdot Q) \neq f_r(P)\cdot f_r(Q)$.


\paragraph{Finding valuable points}

Fortunately, there are some points $r\in \Z_q$ which can be used to create polynomial evaluation maps that 
are ring homomorphisms.
\begin{thm}
  Let $R_q=Z_q[X]/F(X)$ and let $r\in Z_q$. If $F(r)=0$ then $f_r$ is a ring homomorphism.
\end{thm}
\begin{proof}
  $f_r$ preserve the multiplicativeidentity: $f_r(1_{R_q}) = 1(r)=1$.
  Every $f_r$ preserver addition.
  preserving multiplication:
  Let there be $P(X), Q(X) \in R_q$. 
  By definition $P(X)\cdot Q(X) \mod F(X) = P(X)\cdot Q(X) \lfloor \frac{P(X)\cdot Q(X)}{F(X)} \rfloor \cdot F(X)$,
  so $f_r(P(X)\cdot Q(X))= f_r(P(X))cdot f_r(Q(X)) - f_r(\lfloor  \frac{P(X)\cdot Q(X)}{F(X)} \rfloor)\cdot f_r(F(X))$.
  Because $f_r$ is an evaluation map, $f_r(\lfloor  \frac{P(X)\cdot Q(X)}{F(X)} \rfloor)=z$ where is some unknown integral from $Z_q$.
  then $f_r(P(X)\cdot Q(X)) =P(r)\cdot Q(r) - z \cdot 0= P(r)\cdot Q(r)$, as wanted.
\end{proof}

So, by finding the roots of $F(X)$, we can identify potential values where 
the SZ algorithm continues to function effectively. 
However, this set is quite limited (consider the maximum number of roots of the polynomial $F(X)$).

\section{Polynomial addition}\label{polynomials:addition}
\section{Fast Polynomial Multiplications}\label{polynomial:multiplication}\label{section:fft}

We know how to add two polynomials as efficient as possible~\ref{polynomials:addition},
but the simple definition of polynomial multiplication isn't as efficient, and demand $O(n^2)$.
Many complex algorithms make use of polynomial multiplication, and as a result, the algortihm 
we are going to describe was announced as one of the top 10 Algorithms of 20th
Century by the IEEE magazine Computing in Science and Engineering.\cite{814652}


We begin by remembering the point-value representation~\ref{polynomials:point-val-representation}.
In this representation, once can perform both addition and multiplication between two polynomials 
by performing point to point operations (similar to addition in coefficient representation)\todo{provide proof?}.
As a result multiplication and addition in this representation cost $O(n)$.


The naive algorithm of moving from coefficient representation to point-value representation
requires evaluating the polynomials $n+1$ times, and as a result runs in $O(n^2)$.
Thus, to perform polynomial multiplication we'll begin with finding a clever and fast way to move between representations.
\subsection{FFT}
Luckily there is a way to move to and from both representations quickly using the \emph{roots of unity}.

\begin{defn}
An $n$th root of unity is a complex number $\omega$ such that
$\omega^n =1$.
\end{defn}

There are exactly $n$ roots of unity $e^{2\pi i k/n}$ for $k\in[n-1]$ 
($e^{iu}= \cos{u}+i \sin{u}$).

The value $\omega_n=e^{2\pi i/n}$ is the \emph{principal nth root of unity}.
all other roots of unity are powers of $\omega_n$:
 $\omega_n^0, \omega_n^1, \omega_n^2,\dots, \omega_n^{n-1}$.

FFT takes special advantage that these values have special properties.
\begin{lemma}[roots of unity]\label{roots-of-unity-property}
If $z$ is an nth root of unity, then $ a \equiv b \mod n \Rightarrow z^a = z^b$.
\end{lemma}
meaning that $\omega_n^a = \omega_n^b$. one can label specific points on the unit circle over $\C$.

\paragraph{Why is that important?}

In the simplest of cases, we are going to evaluate the polynomial over the following
points $ \{\omega_n^0, \omega_n^1, \omega_n^2,\dots, \omega_n^{n-1}\}$.
Let us view the polynomial evaluation over $n$ such points as a matrix, this presentation
might help the reader see some similarities:


$$
\begin{pmatrix}
  y_0\\
  y_1\\
  \vdots \\
  y_{n-1}\\
\end{pmatrix}
=
\begin{pmatrix}
  1 & x_0 & x_0^2 ,\dots\\
  1 & x_1 & x_1^2, \dots,\\
  \vdots & \vdots & \vdots & \ddots & \\
  1 & x_{n-1} & x_{n-1}^2, \dots,\\
 \end{pmatrix}
 \cdot 
\begin{pmatrix}
  a_0\\
  a_1\\
  \vdots \\
  a_{n-1}\\
\end{pmatrix}
 $$

 Now, let us assign the values:

 \label{dft-matrix}
 $$
\begin{pmatrix}
  p(\omega_n^0)\\
  p(\omega_n^1)\\
  \vdots \\
  \omega_n^2\\
\end{pmatrix}
=
\begin{pmatrix}
  1 & (\omega_n^0)^1 & (\omega_n^0)^2 ,\dots ,(\omega_n^0)^{n-1}\\
  1 & (\omega_n^1)^1 & (\omega_n^1)^2, \dots, (\omega_n^1)^{n-1}\\
  \vdots & \vdots & \vdots & \ddots & \\
  1 & (\omega_n^{n-1}) & (\omega_n^{n-1})^2, \dots,(\omega_n^{n-1})^{n-1}\\
 \end{pmatrix}
 \cdot 
\begin{pmatrix}
  a_0\\
  a_1\\
  \vdots \\
  a_{n-1}\\
\end{pmatrix}
 $$

 The keen looker can see that we can reuse values in this matrix.
 for example, let us look at $(\omega_n^{n-1})^{n-1}$ using \ref{roots-of-unity-property}
 $(\omega_n^{n-1})^{n-1} = (\omega_n)^{n*n-2n+1}=(\omega_n)^{0+0+1}$.  

 Thus we gain a matrix with heavy symmetry, which the FFT can use.

\paragraph{fft main idea}
The FFT method employs a divide-and-conquer strategy, 
using the even-indexed and odd-indexed coefficients of $A(x)$
separately to define the two new polynomials $A^{[0]}(x)$ and $A^{[1]}(x)$ of degree-bound $n/2$.

\begin{align}
  &A^{[0]}(x) =  a_0 +a_2x + a_4x^2 + \dots + a_{n-2}x^{n/2-1}\\
  &A^{[1]}(x) =  a_1 +a_3x + a_5x^2 + \dots + a_{n-1}x^{n/2-1}.
\end{align}


It follows that $A(x)=A^{[0]}(x^2) + x\cdot A^{[1]}(x^2)$.

Thus, the problem of evaluating $A(x)$ at 
$\omega_n^0, \omega_n^1, \omega_n^2,\dots, \omega_n^{n-1}$ 
reduces to evaluating the degree-bound $n/2$ polynomials  $A^{[0]}(x)$ and $A^{[1]}(x)$ at the points
$$(\omega_n^0)^2, (\omega_n^1)^2,\dots, (\omega_n^{n-1})^2$$
and then combining the results according to $A(x)=A^{[0]}(x^2) + x\cdot A^{[1]}(x^2)$.


Using the properties of the root of unity, 
the above values consist not of $n$ distinct values but only of the $n/2$
 complex ($n/2$)th roots of unity, with each root occurring exactly twice!
These subproblems have the same form as the original problem but are half the size.
We can continue in this manner until we evaluate the full polynomial on $n$ distinct values.


\subsection{putting it all together}
\red{there might be some miscalculations here, but the idea is solid.}
As we've seen we can create a matrix with heavy symmetry, called the DFT matrix. 
now, we'd want to capitalize on that and create a recursive algorithm.
To do so, instead of the regular polynomial assignment we've seen above using a matrix,  
we'll shuffle things a bit according to the divide-and-conquer approach from above
$$
F_n \cdot p(x) = 
\begin{pmatrix}
  I_{ \frac{n}{2}} & D_{\frac{n}{2}} \\
  I_{ \frac{n}{2}} & -D_{\frac{n}{2}}
 \end{pmatrix}
 \cdot 
 \begin{pmatrix}
  F_{ \frac{n}{2}} & 0 \\
  0 & F_{ \frac{n}{2}} 
\end{pmatrix}
\cdot 
\begin{pmatrix}
  p_{\text{even coefficient}} \\
  p_{\text{odd coefficient}}
\end{pmatrix}
 $$

 where $F_n$ is the matrix of assignment we've seen in ~\ref{dft-matrix}, and 
 $D_n$ is the following diagonal matrix $$
 \begin{pmatrix}
  1 & 0 &0  & \dots& 0 \\
  0 & \omega_n^1 &0  & \dots& 0 \\
  1 & 0  &\omega_n^2  & \dots& 0 \\
  \vdots & \vdots & \vdots & \vdots &  \\
  0 & 0  &0  & \dots& \omega_n^n-1 \\
 \end{pmatrix} $$


 We can continue this process recursively, to get what we want.
 In each recursive level, the algorithm does at most $O(n)$ work (i.e., multiplying by sparse matrices),
  and due to the logarithmic depth of the recursion, we get $O(n\log{n})$.
x
\label{algorithm:fft}
  \begin{lstlisting}[language=Python,
    caption={fft algorithm in python}
  ]
import numpy as np

def fft(p: np.ndarray) -> np.ndarray:
    n = len(p)
    if n == 1:
        return p
    omega = np.exp(2 * np.pi * 1j / n)

    p_even, p_odd = p[::2], p[1::2] 
    ye, yo = fft(p_even), fft(p_odd)

    # mult the structured left matrix with the recursive result.
    # O(n) work comes from here.
    y = np.zeros(n, dtype=np.complex128)
    for k in range(n // 2):
        y[k] = ye[k] + omega ** k * yo[k]
        y[k + n // 2] = ye[k] - omega ** k * yo[k]
    return y
    \end{lstlisting}
    
    \begin{remark}[Summary]
      Notice that to achieve this result, we needed to reduce the polynomial into
      even and odd parts, and then adjust the assignment matrix using
      the properties of roots of unity. Each part is important in its own way.
    \end{remark}
\subsection{Inverse FFT}\label{ifft}

The DFT matrix is invertible.
Thus one can solve for the vector of coefficients,
by simply inverting the matrix in~\ref{dft-matrix}.

Fortunately, with little tricks we can adjust the inverse of the DFT matrix such that
we can use the same algorithm as above, but instead of
$\omega^k$ we'll use $\omega^{-k}$, and after we are done with the FFT
algorithm, we'll need to normalize it by dividing every coefficient element
by $N$ the size of the polynomial.

 \begin{corollary}
  For FFT, DFT, and NTT we must use specific values to gain speedy interpolation and
  evaluation of the polynomial. 
 \end{corollary}{Conclusin:}
 
\subsection{Number Theoretic Transform (NTT)}\label{ntt}

NTT is a specialized version of the discrete Fourier transform,
in which the coefficients of the polynomial are members of a finite field (or ring)
 containing the right roots of unity.
It can be viewed as an exact version of the complex DFT, avoiding 
round-off errors for exact convolutions of integer sequences.
While Gauss used similar techniques already in~\cite{gaussfft}, laying the groundwork 
for modern FFT algorithms to compute the DFT, and therefore the NTT, 
is usually attributed to Cooley and Tukey's seminal paper \cite{fft}.

NTT can use the same FFT algorithms but requires a 
different DFT matrix~\ref{dft-matrix}.

This fact lies in the foundational theorem of NTT:
\begin{thm}[\cite{ntt}]
  A length $N$ transform having the DFT structure will
  implement cyclic convolution if and only if there exists an 
  inverse of N and an element $\alpha$,
  a root of unity of order $N$ , i.e., $N$ is the least 
  positive integer such that $\alpha^N =1$.
\end{thm}
This is a very general result applying to both \emph{rings and fields} that are
finite or infinite.

It is important to note that $N$ also applies to the length of our polynomial.

\subsubsection{Preliminaries}
I will describe everything in terms of fields, but essentially all of this can be easily generalised to rings 
using the Chinese Remainder Theorem (CRT), as shown below.
\paragraph{\bf Finding Roots of Unity (In Field)}
As described above, an $n$-th root of unity, is an element $\alpha\in Z_q$ 
such that $\alpha^n =1  \mod q $, and for every $0<k<n$ $\alpha^k \neq 1 \mod q$.

\paragraph{Finding roots of unity in a field}

\begin{thm}
  Let $g\in GF(q)$ be a generator, thus
  $\omega_0=g^{(q-1)/n}$ is an $n$-th root of unity.
\end{thm}
\begin{proof}
  \sloppy{
  relying on \autoref{fermats-little-theorem}, we know that $g^{q-1}=1 \mod q$.
  So $\omega_0^n=(g^{(q-1)/n})^n=1$, now we just need to explain away why $\omega_0^k \ne 1$ for 
  every $0<k<n$.
  Let us assume in contradiction that there exists some $k<n$ such that $\omega_0^k=1$, then 
  $\omega_0^k=g^{(q-1/n)\cdot k}=1$, more specifically, $g^c=1$ for $c<q-1$. thus, we have a 
  cycle $(1,g,g^1,...g^{c-1})$, which is less than $q-1$ elements, meaning that $g$ is not 
  a generator, in contradiction.
  }
\end{proof}

Now that we found the first root of unity,
we can find the rest by computing the powers of $\omega_0$ up to $n$.


But how do we find the generator to begin with? 
There is a randomised algorithm to find a generator in an expected 
$\approx O(\sqrt{q-1})+O(\log^2{q}\cdot \log (\log(q-1)))$ time for small primes.
For larger primes it'll longer, but it is still feasible ($O(e^{\log q-1})$).

The algorithm itself is based on factorising $q-1$ into prime factors, and then
check that a random number $g$ is a generator by ensuring $g^{(p-1)/p_i} \not\equiv 1$
for each $p_i$ prime factor of $q-1$ (where $q$ is the field's prime order).


\paragraph{Neat Trick}
\todo{This section would have saved me days of work, had I known it before.}
One can make use of these roots of unity to find the roots of $X^n+1$ and $X^n-1$.
Finding the roots of $X^n-1$ is from the definition! 
Every n-th root of unity is a root of $X^n-1$: $(\omega^k)^n=(\omega^n)^k=1^k=1$.
Substitute $X$ with any n-th root of unity, and you'll get $1-1=0$.

Finding the roots of $X^n+1$ is a bit more tricky,
but it can be done by finding the roots of $X^{2n}-1$ (which should be easy
as we've shown above):
First let's recall that $X^{2n}-1=(X^n+1)(X^n-1)$, then we notice that
we have $n$ roots of $X^n-1$,
and $n$ roots of $X^n+1$. So every one of the roots of $X^{2n}-1$ that is
not a root of $X^n-1$ is a root of $X^n+1$.
How do we find the roots of $X^{2n}-1$? Those are the $2n$-th roots of unity, 
which are easy to find!

\paragraph{\bf Chinese Remainder Theorem (CRT) For Dummies}
CRT is a way of representing elements $x\in Z_q$ (
  where $q=p_1,p_2,\dots,p_k$ and $p_i\in\text{Primes}$)
such that the $x_{CRT}$ is a single and unique representation of $x$ using the 
remainders of each of the prime factors constructing $q$:

$$ x_{CRT} = (x\mod p_1, x\mod p_2 ,\dots x\mod p_k ) $$



Because it is a unique representation, given $x_{CRT}$ and the list of
 primes $(p_1,p_2,\dots,p_k)$ there is an algorithm to solve for $x \in Z_q$
 (i.e., find $x \mod q$).


Another important thing to notice is that if  $x=1 \mod q$
 then $x_{CRT}= (1,1,1,\dots,1)$.


There is more to it honestly, but for our cause, it's enough.

\subsubsection{Putting it all together}

\paragraph[NTT Over Finite Fields]{\bf NTT Over Finite Fields} \label{ntt-field}
When working over a field $GF(p)$ such that $p$ is prime,
the maximal $N$, the array of coefficients representing a
polynomial, is $N_{max}=p-1$, and $N$ is a valid size for NTT
 in this field iff $N|p-1$ (\cite[III. number theoretic transforms]{ntt}). 


To compute the NTT over $GF(p)$, find the $n$th root of unity in $GF(p)$,
 then run any FFT algorithm mod $p$ using the roots
of unity for each level of the recursion.

That is, find $\omega_n=\alpha$ such that $\alpha^N=1 \mod p$, 
then run the (almost) same algorithm in \nameref{algorithm:fft}, 
but change line 7 such that omega is now the $n$-th root of unity in $GF(p)$, and
all operations in the algorithm are now $\mod p$.

\paragraph{\bf iNTT}
Just like in \nameref{ifft}, but to calculate $\omega^-1$, we'll
use the algorithm shown in \autoref{sec:finite-field:extended-euclidean} 
to find the inverse of every element ($x^-1$).



\paragraph[short]{\bf NTT over Rings}
Let $Z_M$ represent a ring of integers ${0,1,2,\dots , M-1}$,
with arithmetics mod $M$.
Where $M$ is constructed the following unique prime factorization
$M=p_{1}^{r_1}\cdot  p_{2}^{r_2} \cdots p_{\ell}^{r_\ell}$
and $p_i$'s are distinct primes (see \cite[NTT sec. III][]{ntt}
 for further explanations).

For NTT to work, the following must hold $\bf N|L(M)$
where $L(M)$ is the greatest common
divisor (gcd) of the $(p_{i}-1)$. That is, 
$L(M):= gcd\{p_1-1,p_2-1,\dots\}$

essentially it means we want $ N$ to divide $p_i-1$.

\begin{corollary}
  $L(M)=N_{max}$. i.e., it defines the maximal $N$ 
  such that we can apply NTT.  
\end{corollary}

Now, Once $M$ and $N$ are known, to perform NTT we just need to be able to find 
$N$-th root of unity. but it's quite hard to find one in $Z_M$.
Luckily, we know it is feasible to do so in a prime field. 
So, let's find $\alpha_i$, the $N$-th root of unity for every $p_i$.
Once we find all of these roots of unity, we know that $\alpha_i^N=1 \mod p_i$ for every $i$.
In other words, $N$ is the least positive integer such that
\begin{align*}
  &\alpha_{CRT}^N = (\alpha_1^N \mod p_1=1, \alpha_2^N \mod p_2=1,\dots)= (1,1,\dots)
\end{align*}

and according to CRT $\alpha^N=1 \mod M$. Thus, solving for $\alpha$ 
means we'll be able to find the $N$-th root of unity in $Z_M$.

The algorithm remains the same as in \nameref{ntt-field} from here on.

\paragraph{Special Features}
Applying the NTT transform provides a cyclic convolution, computing 
$c = a \cdot b \mod (X^n + 1)$  with two polynomials $a$ and $b$ would require 
applying the NTT of length $2n$ and thus $n$ zeros to be appended to each input;
this effectively doubles the length of the inputs and also requires the computation of 
an explicit reduction modulo $X^n +1$. 
To avoid these issues, one can exploit the \emph{negative wrapped convolution} \cite{negantt}.

This exploitation allows us to perform $NTT$ without expanding to $2n$ parameters!
meaning we can send less bandwidth over the network, and perform fewer computations too. 
This trick can be seen in \cite{SEAL}.

\section{Reed-Solomon}
It is fairly simple, but effective algorithm to encode words and be able to deal with erasurs.
\subsection{Reed-Solomon for erasures}
The Reed-Solomon algorithm is a systematic error-correcting code that can correct multiple erasures.
It is based on polynomial arithmetic over a finite field, and it is used in many applications.

The algorithm works as follows:
encode the message in a polynomial of degree bound $k-1$,
and then evaluate the polynomial at $n$ distinct points.

To decode the message, we need to find the polynomial that interpolates the points we received.
Given any $k$ points, once can find a unique polynomial of degree $k-1$ that passes through them
using the Lagrange interpolation formula.


\paragraph{Using NTT for Reed-Solomon} 
The Reed-Solomon algorithm can be accelerated by employing the NTT algorithm (see \autoref{ntt}).
Instead of utilizing any arbitrary $n$ distinct points, we can leverage the $n$-th 
roots of unity (provided they exist within the finite field under consideration).
Now we can encode messages faster.

Decoding can still be done using the Lagrange interpolation formula.
\todo{How to improve decoding speed using NTT!} 


\subsection{Reed-Solomon for errors}
\todo[inline]{This section is not complete}
The Reed-Solomon algorithm can also correct data corruption.


% decoding can be done using the Berlekamp-Welch algorithm, which is a generalization of the Euclidean algorithm.
% It can be used to find the error locations and values, and then correct them.




% \subsection{Naive solution}
% assuming we decide we want our words to be of size $k=4$, and we add additional two redundancies $n=6$. 
% Thus to send it we'll do the following: given a word vector, we'll treat it as coefficients values of a 
% polynomial of degree $k-1$ (because we have k coefficients its a k-1 degree poly), then to encode it, we'll evaluate it on the following $x$ values: $(-1,0,1,2,\dots n)$.


% Now given the 6 values that we received over the network we can decode it as follows: 
% We go over every group of 4 points, and try to reconstruct the polynomial. 

% the polynomial we've reconstructed the most is the correct one.

% \subsection{Systematic Reed-Solomon}
% Assuming we want to be able to correct up to $s$ values, we'd need at least $2s$ redundant symbols.
% So if $k=4$, and $n=6$, we can only recover a single byte.

% It won't work for larger values, like $k=223$, $n=255$ which is a popular value for reed-solomon.
% (Meaning we add 32 more symbols to our message, so we can repair 16 symbols in total).

% The previous algorithm will work, but we'd need to send different values then 
% the original message. It would be best to send $Message||\text{addition code word}$.
% that way,we might be able to avoid reconstruction if we can find quickly that there was no error.

% this is called a Systematic code. 
